# ğŸŒ³ AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM

![](img/cover.png)

> [!WARNING]
> The manuscript is currently under submission and revision. The source code will be released once the manuscript is accepted for publication.

> [!CAUTION]
> Temporary Citation (Preprint) - ArXiv: TODO

## ğŸšœ Overview

AgriGS-SLAM is a Visualâ€“LiDAR SLAM framework designed for robust real-time 3D mapping and reconstruction in agricultural orchards. The system couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering to handle the unique challenges of orchard environments, including repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion.

## ğŸ“ Abstract

Autonomous robots in orchards require real-time
3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visualâ€“LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradientdriven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, backpropagated through the camera projection to tighten geometryappearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance ontractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring
robust multimodal perception.

##  ğŸ“š Citation

If you find this work useful, please cite it once available:

```bibtex
@article{usuelli2025agrigsslam,
  ... TODO
}
```

## ğŸ‘¨â€ğŸŒ¾ Authors

- **Mirko Usuelli**Â¹* and **Matteo Matteucci**Â¹  
  Dipartimento di Bioingegneria, Elettronica e Informazione, Politecnico di Milano, 20133 Milano, Italy  
  {mirko.usuelli, matteo.matteucci}@polimi.it

- **David Rapado-Rincon**Â² and **Gert Kootstra**Â²  
  Agricultural Biosystems Engineering, Wageningen University & Research, 6708 PB Wageningen, The Netherlands  
  {david.rapadorincon, gert.kootstra}@wur.nl

*Corresponding author

## ğŸ™ Acknowledgments

The authors thank the Fruit Research Center (FRC) in Randwijk for access to the orchards. Mirko Usuelli's work was carried out within the Agritech National  Research Center and funded by the EU Next-GenerationEU (PNRR â€“ M4C2, Inv. 1.4 â€“ D.D. 1032 17/06/2022, CN00000022). This manuscript reflects only the authors' views; the EU and Commission are not responsible. Contributions from Matteo Matteucci, Gert Kootstra, and David Rapado-Rincon were co-funded by the EU Digital Europe Programme (AgrifoodTEF, GA NÂº 101100622).
